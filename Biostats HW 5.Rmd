---
title: "P8130 Biostats Methods Homework 5"
author: "Alison Elgass"
output: pdf_document
---

```{r}
library(tidyverse)
library(dplyr)
library(faraway)
library(broom)
library(purrr)
```

# Problem 1
```{r}
states = as_tibble(state.x77) %>% janitor::clean_names()
```

## Part a
```{r}
summary(states)
attach(states)
```

## Part b
```{r}
#par(mfrow=c(1,1))
plot(income, life_exp) #some + linear
hist(income)

plot(illiteracy, life_exp) #linear -, outliers
hist(illiteracy)

plot(murder, life_exp) #linear -
plot(hs_grad, life_exp) #linear +, outliers
```

## Part c
### backward elimination
```{r backward}
#fit regression model with all predictors
mult.fit <- lm(life_exp ~ ., data = states)
tidy(mult.fit)

#eliminate variables by highest p-val
step1 = update(mult.fit, . ~ . -area)
tidy(step1)

step2 = update(step1, . ~ . -illiteracy)
tidy(step2)

step3 = update(step2, . ~ . -income)
tidy(step3) #R2 =0.736, R2adj = 0.7126

step4 = update(step3, . ~ . -population) #close, p-val = 0.052
tidy(step4) #R2 =0.713, R2adj = 0.694  LOWER
```
The step3 model includes: population, murder, hs_grad, frost  
The p-value in this model for population is 0.052.  
Since this is close to the often-used 0.05 threshold, we check the model if population is additonally removed.  
  
This step4 model includes: murder, hs_grad, frost  
However this model has a slightly lower R^2^ (0.713 vs. 0.736) and R^2^ adjusted (0.694 vs. 0.713), so I would go with the step3 model which includes population.  
  
### forward elimination
```{r forward}
#function to nicely extract p-value of last variable from broom::tidy
pvals = function(fitn) {
  p = tidy(fitn)$p.value[nrow(tidy(fitn))]
  p
}

#0. start with single variables
fit1 = lm(life_exp ~ population, data = states)
fit2 = lm(life_exp ~ income, data = states)
fit3 = lm(life_exp ~ illiteracy, data = states)
fit4 = lm(life_exp ~ murder, data = states)
fit5 = lm(life_exp ~ hs_grad, data = states)
fit6 = lm(life_exp ~ frost, data = states)
fit7 = lm(life_exp ~ area, data = states)

fits = tibble(fit1, fit2, fit3, fit4, fit5, fit6, fit7)
map(.x = fits, ~ pvals(.x)) #get all p-values

#1. lowest p-val = murder (fit4)
forward1 = lm(life_exp ~ murder, data = states)
# update forward1 by trying to add each other predictor
fit1 = update(forward1, . ~ . +population)
fit2 = update(forward1, . ~ . +income)
fit3 = update(forward1, . ~ . +illiteracy)
fit4 = update(forward1, . ~ . +hs_grad)
fit5 = update(forward1, . ~ . +frost)
fit6 = update(forward1, . ~ . +area)

fits = tibble(fit1, fit2, fit3, fit4, fit5, fit6)
map(.x = fits, ~ pvals(.x)) #get all p-values

#2. next lowest p-val = hs grad (fit4)
forward2 = update(forward1, . ~ . +hs_grad)
# update forward2 by trying to add each other predictor
fit1 = update(forward2, . ~ . +population)
fit2 = update(forward2, . ~ . +income)
fit3 = update(forward2, . ~ . +illiteracy)
fit4 = update(forward2, . ~ . +frost)
fit5 = update(forward1, . ~ . +area)

fits = tibble(fit1, fit2, fit3, fit4, fit5)
map(.x = fits, ~ pvals(.x)) #get all p-values

#3. next lowest p-val = frost (fit4)
forward3 = update(forward2, . ~ . +frost)
# update forward3 by trying to add each other predictor
fit1 = update(forward3, . ~ . +population)
fit2 = update(forward3, . ~ . +income)
fit3 = update(forward3, . ~ . +illiteracy)
fit4 = update(forward3, . ~ . +area)

fits = tibble(fit1, fit2, fit3, fit4)
map(.x = fits, ~ pvals(.x)) #no significant p-values
#close though- adding population (fit1) p-value = 0.052

summary(forward3) #no population, R2adj = 0.69
summary(fit1) #population added, R2dj = 0.71
```

We run into the same close call with population. The forward3 model includes murder, hs grad, and frost; the p-value for population is 0.052. Again though the model with population included has a higher R^2^ so I choose to go with this one.  
  
The subset includes murder, hs_grad, frost, and population.

### illiteracy vs. hs graduation rate
```{r}
plot(hs_grad, illiteracy)
```
There appears to be a very weak negative relationship (higher graduation rates correlate with lower illiteracy rates aka higher literacy). The subset includes only hs_grad rate.

## Part d
```{r}
aa = leaps::regsubsets(life_exp ~ ., data=states)
bb = summary(aa)

par(mfrow=c(1,2))
plot(1:7, bb$cp, xlab="# of predictors", ylab="Cp Statistic")
plot(1:7, bb$adjr2, xlab="# of predictors", ylab="Adj R2")
```

These plots indicate 4 predictors is optimal, with the lowest C~p~ and highest adjusted R^2^. This confirms my inclination from stepwise procedures to include population as a predictor.

## Part e
My final model, then, would include the following 4 predictors: murder, hs graduation rate, frost, and population.  

```{r}
final = lm(life_exp ~ murder + hs_grad + frost + population)
summary(final)
```

Life expectancy = 71.03 - 0.3X~murder~ + 0.00466X~grad~ - 
0.00594X~frost~ + 0.00005X~population~  
  
## Part f
I would conclude that life expectancy can be predicted best by these variables. Increasing murder rates and frost have a negative effect on life expectancy; for example, we would expect a 1% increase in murder rate to result in a decrease of 0.3 years of life expectancy. Oppositely, high school graduation rate and population have a positive association with life expectancy, though population was a tough call since it may or may not be significant. Overall this model is based only on the data given, which means it's limited in its predictive ability and generalizability, especially since the data is ecological.  


# Problem 2
```{r}
properties = read_csv("./CommercialProperties.csv") %>% janitor::clean_names()
```

## Part a
```{r}
full = lm(rental_rate ~ ., data = properties)
summary(full)
```
All predictors appear to be highly significant except for vacancy rate, which has a p-value of 0.57. The adjusted R^2^ is 0.563, so the model fits okay.

## Part b
```{r}
attach(properties)

par(mfrow=c(1,3))
plot(age, rental_rate)
plot(taxes, rental_rate)
plot(sq_footage, rental_rate)
```
The relationship between age and rental rate seems very week; at first I could not even discern the direction of association. Taxes and square footage are better, both with a clear positive association with rental rate.

## Part c
```{r}
better = lm(rental_rate ~ age + taxes + sq_footage, data = properties)
summary(better)
```

## Part d
```{r}
par(mfrow=c(1,1))
plot(age, rental_rate)

# HIGHER ORDER: age^2 quadratic
properties2 = mutate(properties, age2 = age^2)
quadfit = lm(rental_rate ~ age + taxes + sq_footage + age2,
               data = properties2)
summary(quadfit)


# KNOTS - piecewise linear regression: 2 knots at 5, 10
propertiesK = mutate(properties, 
                     spline_5 = (age - 5) * (age >= 5),
                     spline_10 = (age - 10) * (age >= 10))

piecefit = lm(rental_rate ~ age + taxes + sq_footage + 
                spline_5 + spline_10, data = propertiesK)
summary(piecefit)
```

I tried both a quadratic model and a piecewise model, with splice points at age = 5 and age = 10, which is where I see big clusters. The quadratic seems like the better choice visually and statistically.

## Part e
```{r}
anova(better, quadfit)
```
The adjusted R^2^ for my model in part c (which includes age, taxes, and square footage as predictors) = 0.567, while the adjusted R^2^ for the quadratic model in part d = 0.593 and has a slightly lower residual standard error. We can do an ANOVA test comparing the 2 models where  
H~0~: model 1 (part c) is better  
H~1~: quadratic model (part d) is better  
  
The test stat F = 5.91 and p-value = 0.017, so I would reject the null and say the quadratic model is better.  
  
In subsequent analyses I would also look at other higher order models and possible transformations to age based on the data.

